{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Implementation ( Souce Code )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - _Kranthi Kumar Valaboju_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referred: geeksforgeeks.org/python-random-sample-function\n",
    "# Referred stackoverflow.com/questions/23297569/python-key-error-0\n",
    "\n",
    "import random\n",
    "\n",
    "# Function for Random Sampling\n",
    "\n",
    "def RandomSampler(data,k):\n",
    "    \n",
    "    x=data.sample(k)\n",
    "    y=np.array(data['Price'])\n",
    "    x=np.array(data.drop('Price',axis=1))\n",
    "        \n",
    "    return x,y\n",
    "\n",
    "# SGD Function\n",
    "\n",
    "def StochasticGradientDescent(data_train,learning_rate,learning_rate_factor,n_iter,batch_size):\n",
    "    \n",
    "    # X_train: Training Data\n",
    "    # learning_rate: ~r \n",
    "    # learning_rate_factor: rate at which r has to be reduced\n",
    "    # n_iter: No of iterations\n",
    "    # batch_size: Size to which the Random Sampling has to be done\n",
    "    # Defining Parameters w and b\n",
    "    \n",
    "    col=(data_train.shape[1]-1)\n",
    "    \n",
    "    w=np.zeros(shape=(1,col))\n",
    "    w_plus=np.zeros(shape=(1,col))\n",
    "    b=0\n",
    "\n",
    "    b_plus=0\n",
    "    \n",
    "    # Number of Iterations\n",
    "    \n",
    "    pres_iter=1\n",
    "    \n",
    "    while (pres_iter<(n_iter+1)):\n",
    "        \n",
    "        x,y=RandomSampler(data_train,batch_size)\n",
    "                \n",
    "        # Calculating Partial Derivatives\n",
    "        #             grad(w)= summation over 1 to batch_size : ((-2*x(i))*(y(i))-w_trans*x(i)))\n",
    "        #             grad(b)= summation over 1 to batch_size : ((-2)*(y(i))-w_trans*x(i)))\n",
    "        \n",
    "        w_grad=np.zeros(shape=(1,col))\n",
    "        b_grad=0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "        \n",
    "            w_grad=w_grad+((-2*x[i])*(y[i]-(np.dot(w,x[i])+b)))\n",
    "            b_grad=b_grad+((-2)*(y[i]-(np.dot(w,x[i])+b)))\n",
    "\n",
    "        # Implementing w(j+1)=w(j)-r(grad(w))\n",
    "        #              b(j+1)=b(j)-r(grad(b))\n",
    "        \n",
    "        w_plus=w-((learning_rate)*(w_grad)/(batch_size))\n",
    "        b_plus=b-((learning_rate)*(b_grad)/(batch_size))\n",
    "       \n",
    "        # Checking the Break Condition\n",
    "        if ((w_plus==w).all()):\n",
    "            break\n",
    "        else:\n",
    "            w=w_plus\n",
    "            b=b_plus\n",
    "            learning_rate=(learning_rate)/(learning_rate_factor)\n",
    "            pres_iter+=1\n",
    "     \n",
    "    return w_plus,b_plus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
